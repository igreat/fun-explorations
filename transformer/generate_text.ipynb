{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load toy story script as text into a list\n",
    "text: list[str] = []\n",
    "with open(f\"shakespeare.txt\") as f:\n",
    "    text = list(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up table encoder decoder from letters in words to numbers\n",
    "def make_encoder_decoder(words: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "    letters = sorted(set(\"\".join(words)))\n",
    "    encoder = {letter: i for i, letter in enumerate(letters)}\n",
    "    decoder = {i: letter for i, letter in enumerate(letters)}\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = make_encoder_decoder(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "seq_len = 128\n",
    "model = GPT(len(encoder)).to(device)\n",
    "checkpoint = torch.load(\"checkpoint.pt\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random sequence of letters\n",
    "@torch.no_grad()\n",
    "def generate_text(model, start_text, max_len=200):\n",
    "    model.eval()\n",
    "    text = start_text\n",
    "    for i in range(max_len):\n",
    "        x = torch.tensor([encoder[letter] for letter in text[-seq_len:]], dtype=torch.long).to(\"mps\")\n",
    "        x = x.unsqueeze(0)\n",
    "        logits = model(x)\n",
    "        logits = logits[:, -1, :]\n",
    "        # sample from the distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        letter = torch.multinomial(probs, 1).squeeze(0)[-1]\n",
    "        text += decoder[letter.item()]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random sequence of letters\n",
    "print(generate_text(model, \"I am a toy\", max_len=200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b69368623ed639ee298e27c6826506e73bcdec6b9db0563c29b08757f48e8ce8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
